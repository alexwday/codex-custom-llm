# Mock Mode (for local development without enterprise dependencies)
# Set to 'true' on personal computer, 'false' on work computer
MOCK_MODE=true

# OAuth Configuration (required when MOCK_MODE=false)
OAUTH_ENDPOINT=https://your-oauth-endpoint.company.com/oauth/token
OAUTH_CLIENT_ID=your-client-id
OAUTH_CLIENT_SECRET=your-client-secret

# LLM API Configuration
# For mock mode testing, you can use OpenAI's endpoint or keep the placeholder
# On work computer, change this to your internal LLM endpoint
LLM_API_BASE_URL=https://api.openai.com/v1
LLM_MODEL_NAME=gpt-4-internal

# Proxy Mode: Route through local proxy to log all API calls
# Set to 'true' to use proxy_server.py (run it separately first!)
# When enabled, Codex calls localhost:8889 which forwards to real endpoint
PROXY_MODE=false

# Token refresh interval in seconds (default: 900 = 15 minutes)
TOKEN_REFRESH_INTERVAL=900

# Codex Configuration
# Wire API: 'chat' for OpenAI-compatible, 'responses' for newer API
WIRE_API=chat

# Max tokens for responses (important if your endpoint has low defaults)
# Set this to increase response length. Your model supports up to 256000 input tokens.
# Common values: 4096, 8192, 16384, or higher
MAX_TOKENS=4096

# Optional: Azure-style query parameters (e.g., api-version=2024-01-01)
# QUERY_PARAMS=api-version=2024-01-01

# Verbose mode: Show detailed logging of API calls and wrapper activity
# Set to 'true' to see real-time monitoring of what's happening
VERBOSE_MODE=false
